# Kaggle 官方教程：机器学习入门6 随机森林
> 原文：[Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning) > [Random Forests](https://www.kaggle.com/dansbecker/random-forests)
> 
> 译者：[Leytton](https://github.com/Leytton)
> 
> 协议：[CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/)

PS：水平有限，欢迎交流指正（Leytton@126.com）

# 1、介绍
`决策树`给你留下一个难题。一颗较深、叶子多的树将会`过拟合`，因为每一个预测都来自叶子上仅有的几个历史训练数据。一颗较浅、叶子少的树将会`欠拟合`，因为它不能在原始数据中捕捉到那么多的差异。

即使是当今最精良的建模技术，也面临着拟合不足和拟合过度之间问题。但是，许多模型通过一些不错的点子来提升效果。我们将以`随机森林`为例。

`随机森林`使用了许多树，它通过对每棵成分树的预测进行平均来进行预测。它通常比单个决策树具有更好的预测精度，并且使用默认参数效果也不错。如果继续建模，你可以学习更多具有更好性能的模型，但是其中许多模型效果受调参影响特别大。

# 2、案例
你已经多次看到加载数据的代码。数据加载后，我们将得到以下变量：
 - train_X
 - val_X
 -  train_y
 -  val_y

```python
import pandas as pd 
# 加载数据
melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'
melbourne_data = pd.read_csv(melbourne_file_path) 
# 过滤缺失数据
melbourne_data = melbourne_data.dropna(axis=0)
# Choose target and features
y = melbourne_data.Price
melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', 
                        'YearBuilt', 'Lattitude', 'Longtitude']
X = melbourne_data[melbourne_features]

from sklearn.model_selection import train_test_split

# 将数据分割为训练和验证数据，都有特征和预测目标值
# 分割基于随机数生成器。为random_state参数提供一个数值可以保证每次得到相同的分割
# 执行下面代码
train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)
```

我们构建了一个随机森林模型，类似于我们在scikit-learn中构建决策树的方法——这次使用的是`RandomForestRegressor`类，而不是`DecisionTreeRegressor`。

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

forest_model = RandomForestRegressor(random_state=1)
forest_model.fit(train_X, train_y)
melb_preds = forest_model.predict(val_X)
print(mean_absolute_error(val_y, melb_preds))
```
输出结果：
```
202888.18157951365

/opt/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.
  "10 in version 0.20 to 100 in 0.22.", FutureWarning)
```
# 3、结论
可能还有进一步改进的空间，但是这比最佳决策树250,000的误差有很大的改进。你可以修改一些参数来提升随机森林的性能，就像我们改变单个决策树的最大深度一样。但是，随机森林模型的最佳特性之一是，即使没有这种调优，它们通常也可以正常工作。

你很快就会了解`XGBoost`模型，它在使用正确的参数进行调优时提供了更好的性能(但是需要一些技巧才能获得正确的模型参数)。

# 4、去吧，皮卡丘
尝试自己[使用一个随机森林模型](https://www.kaggle.com/kernels/fork/1259186)，看看它对你的模型有多大的改进。

